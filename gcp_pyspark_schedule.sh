gcloud dataproc jobs submit pyspark --cluster=cluster-iz-dplr --region=us-central1 --properties="spark.driver.memory=2g","spark.executor.memory=2g","spark.executor.instances=4","spark.executor.cores=2","spark.submit.deployMode=client","spark.sql.shuffle.partitions=10","spark.shuffle.spill.compress=true" /home/cloudprathab/project/Usecase4_GcpGcsReadWritehive_cloud.py
